{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690dd616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ee3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# paths for datasets\n",
    "RAVDESS_PATH = 'ravdess/audio_speech_actors_01-24/'\n",
    "CREMA_PATH = 'cremad/AudioWAV/'\n",
    "AUDIO_EMOTIONS_PATH = 'audio emotions/Emotions/'\n",
    "\n",
    "# Loading RAVDESS dataset into DataFrame\n",
    "def load_ravdess(path):\n",
    "    directory_list = os.listdir(path)\n",
    "    file_emotion, file_path = [], []\n",
    "    \n",
    "    for actor_dir in directory_list:\n",
    "        actor_path = os.path.join(path, actor_dir)\n",
    "        actor_files = os.listdir(actor_path)\n",
    "        \n",
    "        for file in actor_files:\n",
    "            filename_parts = file.split('.')[0].split('-')\n",
    "            if len(filename_parts) < 3:\n",
    "                continue  \n",
    "            emotion = int(filename_parts[2])\n",
    "            \n",
    "            file_emotion.append(emotion)\n",
    "            file_path.append(os.path.join(actor_path, file))\n",
    "    \n",
    "    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "    path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "    return pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Loading CREMA dataset into DataFrame\n",
    "def load_crema(path):\n",
    "    directory_list = os.listdir(path)\n",
    "    file_emotion, file_path = [], []\n",
    "\n",
    "    for file in directory_list:\n",
    "        file_path.append(os.path.join(path, file))\n",
    "        part = file.split('_')[2]\n",
    "\n",
    "        if part == 'SAD':\n",
    "            file_emotion.append('sad')\n",
    "        elif part == 'ANG':\n",
    "            file_emotion.append('angry')\n",
    "        elif part == 'DIS':\n",
    "            file_emotion.append('disgust')\n",
    "        elif part == 'FEA':\n",
    "            file_emotion.append('fear')\n",
    "        elif part == 'HAP':\n",
    "            file_emotion.append('happy')\n",
    "        elif part == 'NEU':\n",
    "            file_emotion.append('neutral')\n",
    "        else:\n",
    "            file_emotion.append('Unknown')\n",
    "\n",
    "    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "    path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "    return pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Loading audio emotions dataset into DataFrame\n",
    "def load_audio_dataset(path):\n",
    "    emotions = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "    file_emotion, file_path = [], []\n",
    "\n",
    "    for emotion in emotions:\n",
    "        emotion_folder = os.path.join(path, emotion)\n",
    "        if not os.path.exists(emotion_folder):\n",
    "            continue\n",
    "        files = os.listdir(emotion_folder)\n",
    "        for file in files:\n",
    "            file_path.append(os.path.join(emotion_folder, file))\n",
    "            file_emotion.append(emotion)\n",
    "\n",
    "    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "    path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "    return pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Loading RAVDESS, CREMA, and AUDIO emotions datasets into DataFrames\n",
    "ravdess_df = load_ravdess(RAVDESS_PATH)\n",
    "crema_df = load_crema(CREMA_PATH)\n",
    "audio_dataset_df = load_audio_dataset(AUDIO_EMOTIONS_PATH)\n",
    "\n",
    "# Concatenate all datasets\n",
    "all_datasets_df = pd.concat([ravdess_df, crema_df, audio_dataset_df], ignore_index=True)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "all_datasets_df['label_encoded'] = label_encoder.fit_transform(all_datasets_df['Emotions'].astype(str))\n",
    "\n",
    "# Train-test split\n",
    "train_df, val_df = train_test_split(all_datasets_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_mfcc_features(audio_path):\n",
    "    data, sample_rate = librosa.load(audio_path, duration=2.5, offset=0.6)\n",
    "    mfcc_features = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=20)\n",
    "    if mfcc_features.shape[1] < 100:\n",
    "        pad_width = 100 - mfcc_features.shape[1]\n",
    "        mfcc_features = np.pad(mfcc_features, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc_features = mfcc_features[:, :100]  # Truncate to 100 frames if longer\n",
    "    return mfcc_features\n",
    "\n",
    "# Apply feature extraction\n",
    "train_df['features'] = train_df['Path'].apply(extract_mfcc_features)\n",
    "val_df['features'] = val_df['Path'].apply(extract_mfcc_features)\n",
    "\n",
    "# Converting features to numpy arrays\n",
    "features_train = np.array(train_df['features'].tolist())\n",
    "features_val = np.array(val_df['features'].tolist())\n",
    "labels_train = np.array(train_df['label_encoded'])\n",
    "labels_val = np.array(val_df['label_encoded'])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "features_train_scaled = scaler.fit_transform(features_train.reshape(features_train.shape[0], -1))\n",
    "features_val_scaled = scaler.transform(features_val.reshape(features_val.shape[0], -1))\n",
    "\n",
    "# Reshape features back to (20, 100)\n",
    "features_train_scaled = features_train_scaled.reshape(features_train_scaled.shape[0], 20, 100)\n",
    "features_val_scaled = features_val_scaled.reshape(features_val_scaled.shape[0], 20, 100)\n",
    "\n",
    "# model build\n",
    "audio_input = Input(shape=(20, 100))\n",
    "conv1d_1 = Conv1D(64, 3, activation='relu')(audio_input)\n",
    "conv1d_2 = Conv1D(64, 3, activation='relu')(conv1d_1)\n",
    "maxpool1d_1 = MaxPooling1D(pool_size=2)(conv1d_2)\n",
    "conv1d_3 = Conv1D(128, 3, activation='relu')(maxpool1d_1)\n",
    "conv1d_4 = Conv1D(128, 3, activation='relu')(conv1d_3)\n",
    "maxpool1d_2 = MaxPooling1D(pool_size=2)(conv1d_4)\n",
    "flatten1d_1 = Flatten()(maxpool1d_2)\n",
    "dense1 = Dense(256, activation='relu')(flatten1d_1)\n",
    "batch_norm_1 = BatchNormalization()(dense1)\n",
    "dropout1 = Dropout(0.5)(batch_norm_1)\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout1) \n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=audio_input, outputs=output)\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(features_train_scaled, labels_train,\n",
    "                    validation_data=(features_val_scaled, labels_val),\n",
    "                    epochs=50, batch_size=128)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model.save(\"speech_emotion_model.h5\")\n",
    "print(\"Model saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a84fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
