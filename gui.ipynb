{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42735c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 301ms/step\n",
      "Recording...\n",
      "Finished recording.\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Recording...\n",
      "Finished recording.\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "Recording...\n",
      "Finished recording.\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Recording...\n",
      "Finished recording.\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Recording...\n",
      "Finished recording.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Recording...\n",
      "Finished recording.\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import pyaudio\n",
    "import wave\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Constants for speech emotion model\n",
    "MFCC_LENGTH = 20\n",
    "MAX_PAD_LEN = 100\n",
    "AUDIO_DURATION = 2\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "\n",
    "# Constants for facial expression model\n",
    "IMAGE_SIZE = (48, 48)\n",
    "NUM_CLASSES = 7  \n",
    "NUM_CHANNELS = 1  \n",
    "\n",
    "# Loading trained models\n",
    "speech_model = load_model('speech_emotion_model.h5')\n",
    "facial_expression_model = load_model('facial_emotion_model.h5')\n",
    "\n",
    "# emotion labels\n",
    "emotion_labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
    "expression_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "\n",
    "# record audio and predict emotion from speech\n",
    "def predict_speech_emotion():\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(format=pyaudio.paInt16, channels=1, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "        frames = []\n",
    "        print(\"Recording...\")\n",
    "        for _ in range(0, int(RATE / CHUNK * AUDIO_DURATION)):\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "        print(\"Finished recording.\")\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        wf = wave.open('temp_audio.wav', 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        wf.close()\n",
    "\n",
    "        signal, sr = librosa.load('temp_audio.wav', duration=AUDIO_DURATION, sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=MFCC_LENGTH)\n",
    "        # Padding or cropping the features to match the required length\n",
    "        if mfcc.shape[1] < MAX_PAD_LEN:\n",
    "            mfcc_padded = np.pad(mfcc, ((0, 0), (0, MAX_PAD_LEN - mfcc.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfcc_padded = mfcc[:, :MAX_PAD_LEN]\n",
    "        mfcc_input = np.expand_dims(mfcc_padded, axis=0)\n",
    "        prediction = speech_model.predict(mfcc_input)\n",
    "        max_index = np.argmax(prediction)\n",
    "        if max_index in emotion_labels:\n",
    "            emotion_label = emotion_labels[max_index]\n",
    "        else:\n",
    "            emotion_label = \"Unknown\"\n",
    "        return emotion_label\n",
    "    except Exception as e:\n",
    "        print(\"Error in predict_speech_emotion:\", e)\n",
    "        return \"Error\"\n",
    "\n",
    "# Function to predict facial expression from image\n",
    "def predict_facial_expression(image):\n",
    "    try:\n",
    "        # Convert image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Resize and preprocess image\n",
    "        resized_image = cv2.resize(gray_image, IMAGE_SIZE)\n",
    "        resized_image = np.expand_dims(resized_image, axis=-1)\n",
    "        resized_image = np.expand_dims(resized_image, axis=0)\n",
    "        resized_image = resized_image.astype('float32') / 255.0\n",
    "        \n",
    "        # Predict emotion\n",
    "        prediction = facial_expression_model.predict(resized_image)\n",
    "        max_index = np.argmax(prediction)\n",
    "        if max_index in expression_labels:\n",
    "            expression_label = expression_labels[max_index]\n",
    "        else:\n",
    "            expression_label = \"Unknown\"\n",
    "        return expression_label\n",
    "    except Exception as e:\n",
    "        print(\"Error in predict_facial_expression:\", e)\n",
    "        return \"Error\"\n",
    "\n",
    "# Function to continuously capture frames from webcam and update GUI\n",
    "def update_gui():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title(\"Emotion Recognition\")\n",
    "    \n",
    "    frame_label = tk.Label(root, text=\"Facial Expression: \")\n",
    "    frame_label.pack()\n",
    "    frame_image_label = tk.Label(root)\n",
    "    frame_image_label.pack()\n",
    "\n",
    "    speech_label = tk.Label(root, text=\"Speech Emotion: \")\n",
    "    speech_label.pack()\n",
    "    \n",
    "    expression_label = tk.Label(root, text=\"\")\n",
    "    expression_label.pack()\n",
    "    \n",
    "    speech_emotion_label = tk.Label(root, text=\"\")\n",
    "    speech_emotion_label.pack()\n",
    "\n",
    "    # Function to quit the application\n",
    "    def quit_application():\n",
    "        cap.release() \n",
    "        cv2.destroyAllWindows()\n",
    "        root.quit()\n",
    "        root.destroy()\n",
    "\n",
    "    # Button to quit the application\n",
    "    quit_button = tk.Button(root, text=\"Quit\", command=quit_application)\n",
    "    quit_button.pack()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame is not None:\n",
    "            # facial expression label\n",
    "            facial_expression = predict_facial_expression(frame)\n",
    "            expression_label.config(text=\"Facial Expression: {}\".format(facial_expression))\n",
    "\n",
    "            # speech emotion label\n",
    "            speech_emotion = predict_speech_emotion()\n",
    "            speech_emotion_label.config(text=\"Speech Emotion: {}\".format(speech_emotion))\n",
    "\n",
    "            # Display frame in GUI\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            frame = ImageTk.PhotoImage(frame)\n",
    "            frame_image_label.configure(image=frame)\n",
    "            frame_image_label.image = frame\n",
    "\n",
    "        root.update_idletasks()\n",
    "        root.update()\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    update_gui()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2c15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0fc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa2808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cac7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c8a55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f876e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc5653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
