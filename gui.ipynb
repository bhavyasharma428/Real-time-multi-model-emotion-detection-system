{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42735c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "Finished recording.\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "Recording...\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import pyaudio\n",
    "import wave\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import warnings\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "MFCC_LENGTH = 20\n",
    "MAX_PAD_LEN = 100\n",
    "AUDIO_DURATION = 2\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "\n",
    "IMAGE_SIZE = (48, 48)\n",
    "NUM_CLASSES = 7\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "speech_model = load_model('speech_emotion_model.h5')\n",
    "facial_expression_model = load_model('facial_emotion_model.h5')\n",
    "#for speech\n",
    "emotion_labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
    "#for facial\n",
    "expression_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "\n",
    "audio_queue = queue.Queue()\n",
    "stop_recording = False\n",
    "\n",
    "def record_audio():\n",
    "    global stop_recording\n",
    "    try:\n",
    "        while not stop_recording:\n",
    "            p = pyaudio.PyAudio()\n",
    "            stream = p.open(format=pyaudio.paInt16, channels=1, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "            frames = []\n",
    "            print(\"Recording...\")\n",
    "            for _ in range(0, int(RATE / CHUNK * AUDIO_DURATION)):\n",
    "                if stop_recording:\n",
    "                    break\n",
    "                data = stream.read(CHUNK)\n",
    "                frames.append(data)\n",
    "            print(\"Finished recording.\")\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            p.terminate()\n",
    "\n",
    "            if not stop_recording:\n",
    "                wf = wave.open('temp_audio.wav', 'wb')\n",
    "                wf.setnchannels(1)\n",
    "                wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "                wf.setframerate(RATE)\n",
    "                wf.writeframes(b''.join(frames))\n",
    "                wf.close()\n",
    "\n",
    "                audio_queue.put('temp_audio.wav')\n",
    "    except Exception as e:\n",
    "        print(\"Error in record_audio:\", e)\n",
    "        audio_queue.put('error')\n",
    "\n",
    "def predict_facial_expression(image):\n",
    "    try:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        resized_image = cv2.resize(gray_image, IMAGE_SIZE)\n",
    "        resized_image = np.expand_dims(resized_image, axis=-1)\n",
    "        resized_image = np.expand_dims(resized_image, axis=0)\n",
    "        resized_image = resized_image.astype('float32') / 255.0\n",
    "        \n",
    "        prediction = facial_expression_model.predict(resized_image)\n",
    "        max_index = np.argmax(prediction)\n",
    "        if max_index in expression_labels:\n",
    "            expression_label = expression_labels[max_index]\n",
    "        else:\n",
    "            expression_label = \"Unknown\"\n",
    "        return expression_label\n",
    "    except Exception as e:\n",
    "        print(\"Error in predict_facial_expression:\", e)\n",
    "        return \"Error\"\n",
    "\n",
    "def combine_emotions(facial_emotion, speech_emotion):\n",
    "    # a mapping between individual emotions to combined emotions\n",
    "    combined_emotions_mapping = {\n",
    "        ('angry', 'angry'): 'angry',\n",
    "        ('angry', 'disgust'): 'angry',\n",
    "        ('angry', 'fear'): 'angry',\n",
    "        ('angry', 'happy'): 'angry',\n",
    "        ('angry', 'neutral'): 'angry',\n",
    "        ('angry', 'sad'): 'angry',\n",
    "        ('angry', 'surprise'): 'angry',\n",
    "        ('disgust', 'angry'): 'angry',\n",
    "        ('disgust', 'disgust'): 'disgust',\n",
    "        ('disgust', 'fear'): 'disgust',\n",
    "        ('disgust', 'happy'): 'disgust',\n",
    "        ('disgust', 'neutral'): 'disgust',\n",
    "        ('disgust', 'sad'): 'disgust',\n",
    "        ('disgust', 'surprise'): 'disgust',\n",
    "        ('fear', 'angry'): 'angry',\n",
    "        ('fear', 'disgust'): 'disgust',\n",
    "        ('fear', 'fear'): 'fear',\n",
    "        ('fear', 'happy'): 'fear',\n",
    "        ('fear', 'neutral'): 'fear',\n",
    "        ('fear', 'sad'): 'fear',\n",
    "        ('fear', 'surprise'): 'fear',\n",
    "        ('happy', 'angry'): 'angry',\n",
    "        ('happy', 'disgust'): 'disgust',\n",
    "        ('happy', 'fear'): 'fear',\n",
    "        ('happy', 'happy'): 'happy',\n",
    "        ('happy', 'neutral'): 'happy',\n",
    "        ('happy', 'sad'): 'sad',\n",
    "        ('happy', 'surprise'): 'surprise',\n",
    "        ('neutral', 'angry'): 'angry',\n",
    "        ('neutral', 'disgust'): 'disgust',\n",
    "        ('neutral', 'fear'): 'fear',\n",
    "        ('neutral', 'happy'): 'happy',\n",
    "        ('neutral', 'neutral'): 'neutral',\n",
    "        ('neutral', 'sad'): 'sad',\n",
    "        ('neutral', 'surprise'): 'surprise',\n",
    "        ('sad', 'angry'): 'angry',\n",
    "        ('sad', 'disgust'): 'disgust',\n",
    "        ('sad', 'fear'): 'fear',\n",
    "        ('sad', 'happy'): 'sad',\n",
    "        ('sad', 'neutral'): 'sad',\n",
    "        ('sad', 'sad'): 'sad',\n",
    "        ('sad', 'surprise'): 'sad',\n",
    "        ('surprise', 'angry'): 'angry',\n",
    "        ('surprise', 'disgust'): 'disgust',\n",
    "        ('surprise', 'fear'): 'fear',\n",
    "        ('surprise', 'happy'): 'surprise',\n",
    "        ('surprise', 'neutral'): 'surprise',\n",
    "        ('surprise', 'sad'): 'sad',\n",
    "        ('surprise', 'surprise'): 'surprise',\n",
    "    }\n",
    "    \n",
    "    # Combine the facial and speech emotions\n",
    "    combined_emotion = combined_emotions_mapping.get((facial_emotion, speech_emotion), 'unknown')\n",
    "    return combined_emotion\n",
    "\n",
    "def update_gui():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title(\"Emotion Recognition\")\n",
    "    \n",
    "    frame_label = tk.Label(root, text=\"Facial Expression: \")\n",
    "    frame_label.pack()\n",
    "    frame_image_label = tk.Label(root)\n",
    "    frame_image_label.pack()\n",
    "\n",
    "    speech_label = tk.Label(root, text=\"Speech Emotion: \")\n",
    "    speech_label.pack()\n",
    "    \n",
    "    expression_label = tk.Label(root, text=\"\")\n",
    "    expression_label.pack()\n",
    "    \n",
    "    speech_emotion_label = tk.Label(root, text=\"\")\n",
    "    speech_emotion_label.pack()\n",
    "\n",
    "    combined_emotion_label = tk.Label(root, text=\"\")\n",
    "    combined_emotion_label.pack()\n",
    "\n",
    "    def quit_application():\n",
    "        global stop_recording\n",
    "        stop_recording = True\n",
    "\n",
    "        # Wait for the recording thread to stop\n",
    "        audio_thread.join()\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        root.quit()\n",
    "        root.destroy()\n",
    "\n",
    "    quit_button = tk.Button(root, text=\"Quit\", command=quit_application)\n",
    "    quit_button.pack()\n",
    "    \n",
    "    def on_closing():\n",
    "        global stop_recording\n",
    "        stop_recording = True\n",
    "\n",
    "        # Wait for the recording thread to stop\n",
    "        audio_thread.join()\n",
    "        cap.release()\n",
    "        root.destroy()\n",
    "\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "\n",
    "    def update():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            root.after(10, update)\n",
    "            return\n",
    "        \n",
    "        facial_expression = predict_facial_expression(frame)\n",
    "        expression_label.config(text=\"Facial Expression: {}\".format(facial_expression))\n",
    "\n",
    "        if not audio_queue.empty():\n",
    "            audio_file = audio_queue.get()\n",
    "            if audio_file == 'error':\n",
    "                speech_emotion_label.config(text=\"Speech Emotion: Error\")\n",
    "            else:\n",
    "                signal, sr = librosa.load(audio_file, duration=AUDIO_DURATION, sr=None)\n",
    "                mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=MFCC_LENGTH)\n",
    "                if mfcc.shape[1] < MAX_PAD_LEN:\n",
    "                    mfcc_padded = np.pad(mfcc, ((0, 0), (0, MAX_PAD_LEN - mfcc.shape[1])), mode='constant')\n",
    "                else:\n",
    "                    mfcc_padded = mfcc[:, :MAX_PAD_LEN]\n",
    "                mfcc_input = np.expand_dims(mfcc_padded, axis=0)\n",
    "                prediction = speech_model.predict(mfcc_input)\n",
    "                max_index = np.argmax(prediction)\n",
    "                if max_index in emotion_labels:\n",
    "                    emotion_label = emotion_labels[max_index]\n",
    "                else:\n",
    "                    emotion_label = \"Unknown\"\n",
    "                speech_emotion_label.config(text=\"Speech Emotion: {}\".format(emotion_label))\n",
    "\n",
    "                combined_emotion = combine_emotions(facial_expression.lower(), emotion_label.lower())\n",
    "                combined_emotion_label.config(text=\"Combined Emotion: {}\".format(combined_emotion))\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        imgtk = ImageTk.PhotoImage(image=frame)\n",
    "        frame_image_label.imgtk = imgtk\n",
    "        frame_image_label.config(image=imgtk)\n",
    "        \n",
    "        root.after(10, update)  # Update every 10 milliseconds\n",
    "\n",
    "    root.after(10, update)  # Start the update loop\n",
    "    root.mainloop()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    global audio_thread\n",
    "    audio_thread = threading.Thread(target=record_audio)\n",
    "    audio_thread.start()\n",
    "\n",
    "    gui_thread = threading.Thread(target=update_gui)\n",
    "    gui_thread.start()\n",
    "\n",
    "    # Join the audio thread to ensure it finishes before exiting\n",
    "    audio_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2c15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0fc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa2808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b4072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
